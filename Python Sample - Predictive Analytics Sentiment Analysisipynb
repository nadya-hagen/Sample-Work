{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analytics Assignment Group 17\n",
    "##### Group Members:\n",
    "| Name | SNR |\n",
    "| :---- | :--- |\n",
    "|Nadya Hagen | 2049115 |\n",
    "| KenÃ©z KovÃ¡cs | 2040678 |\n",
    "| Aryo Bimo Nugroho | 2039696 |\n",
    "| Sheva Sonia Rahmani | 2075109 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is a model that aims to predict the direction of the movement of the price of a dogecoin based on Elon Musk's tweets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/nadya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading libraries, getting resources:\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import csv\n",
    "import random\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data initialization\n",
    "We expect the user of the model to know their dataset well, the code itself does not have data preparative features. In our case the raw data was processed using mostly pandas and excel features, we felt like the code needn't be  included here as it is not remotely reproducible. We have however included our pandas manipulations in another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_fluctuations.csv', delimiter = ',') # Here we import the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the functions that classify the data\n",
    "We created a total of 4 functions which help us to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the function that makes a list of lists of words of the tweets \n",
    "\n",
    "def into_words(tweetcolumn):\n",
    "    \"\"\"This function creates a list of lists of the words in the tweets.\"\"\"\n",
    "    wordlist = []\n",
    "    for tweet_sentence in tweetcolumn:\n",
    "            tweet_sentence_string_lower = str(tweet_sentence).lower()\n",
    "            tweet_words = tweet_sentence_string_lower.split()\n",
    "            wordlist.append(tweet_words)\n",
    "    return(wordlist)\n",
    "\n",
    "# We create the function that removes stopwords from the list, thus creating a stopword-free list of words\n",
    "\n",
    "def filterstopwords(wordlist):\n",
    "    \"\"\"This function removes stopwords from the list of words in the tweets to create a \n",
    "    stopword-free list of words.\"\"\"\n",
    "    filtered_wordlist = []\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    for lists in wordlist:\n",
    "        for words in lists:\n",
    "            if  words.lower() not in stop_words:\n",
    "                \n",
    "                filtered_wordlist.append(words.lower())\n",
    "    return(filtered_wordlist)\n",
    "\n",
    "# We create the function that makes a list of the top 3000 words used by Elon Musk in his tweets\n",
    "\n",
    "def top3kwordizer(filtered_wordlist):\n",
    "    \"\"\"This function creates a list of the top 3000 words which Elon Musk uses in his tweets.\"\"\"\n",
    "    all_words = nltk.FreqDist(filtered_wordlist)\n",
    "    all_features = list(all_words)[:3000]\n",
    "    return all_features\n",
    "\n",
    "# We create the function that makes the list of lists that contain dictionaries with the features\n",
    "\n",
    "def list_of_dicts(tweet_sentence, all_features):\n",
    "    \"\"\"This function creates a list of lists which contain dictionaries with the features which will\n",
    "    be used for the model.\"\"\"\n",
    "    tweet_words = set(tweet_sentence)\n",
    "    features = {}\n",
    "    for word in all_features:\n",
    "        if word in tweet_words:\n",
    "            features['contains({})'.format(word)] = True\n",
    "        else:\n",
    "            features['contains({})'.format(word)] = False\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset to classified set of data\n",
    "Here we use the functions which we have defined above to go from our original dataset to a classified set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the functions one by one to classify the data\n",
    "wordized_list = into_words(data['tweet']) # we have the list of lists of words\n",
    "filteredwords = filterstopwords(wordized_list) # we have the list of all words filtered free of stopwords\n",
    "top3kwords = top3kwordizer(filteredwords) # we have the top 3000 words from the list above\n",
    "dictslist = list() # we get the list of list of dictionaries containing the features and if theyre in the tweet or not\n",
    "for tweet in wordized_list: \n",
    "        dictslist.append(list_of_dicts(tweet, top3kwords))\n",
    "        \n",
    "# Here we use pandas' dataframe feature in order to build our classified dataset\n",
    "\n",
    "workingdataframe = pd.DataFrame()\n",
    "workingdataframe['features'] = (pd.Series(list(dictslist)))\n",
    "workingdataframe['direction'] = data['direction']\n",
    "\n",
    "# Creating the list of lists that the model will train on\n",
    "\n",
    "finaldata = workingdataframe.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "All that is left to do is to train the model. We first create a training data set to train our model upon and a test data set to test the model's accuracy on. Because our data is descending in its date, we have decided to create the training set based on the data at the bottom of the file and vice versa for the test set. We made this decision as Elon Musk's tweets had more traction on Dogecoin when he first started tweeting about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is :  0.538\n",
      "Most Informative Features\n",
      "     contains(@tashaark) = True                0 : 1      =     11.1 : 1.0\n",
      "         contains(tests) = True                0 : 1      =      8.1 : 1.0\n",
      "            contains(ðŸ”¥ðŸ”¥) = True                1 : 0      =      7.8 : 1.0\n",
      "contains(@teslaratiteam) = True                1 : 0      =      7.0 : 1.0\n",
      "      contains(however,) = True                1 : 0      =      7.0 : 1.0\n",
      "contains(@justpaulinelol) = True                0 : 1      =      6.9 : 1.0\n",
      "           contains(pcr) = True                0 : 1      =      6.3 : 1.0\n",
      "         contains(turns) = True                0 : 1      =      6.3 : 1.0\n",
      "     contains(@aarons5_) = True                1 : 0      =      6.3 : 1.0\n",
      "contains(@caspar_stanley) = True                0 : 1      =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "train_set = finaldata[500:] # Here we select the bottom 3479 tweets/rows in the dataset\n",
    "test_set = finaldata[:500] # Here we select the top 500 tweets/rows in the dataset\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "acc = nltk.classify.accuracy(classifier, test_set) # Here we define the accuracy measure of the model\n",
    "print(\"The accuracy of the model is : \", acc)\n",
    "classifier.show_most_informative_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
